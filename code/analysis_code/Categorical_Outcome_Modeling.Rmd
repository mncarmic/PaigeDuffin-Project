---
title: "Categorical_Outcomes"
author: "Paige Duffin"
date: "11/7/2019"
output: html_document
---

# Exploratory Analysis {.tabset}

## Preparing Workspace

Load necessary packages
```{r}
library(caret) # Classification and Regression Training
library(corrplot)
library(dplyr) # A Grammar of Data Manipulations
library(forcats) # Tools for Working with Categorical Variables (Factors)
library(gdata) # Various R Programming Tools for Data Manipulation
library(ggplot2) # Create Elegant Data Visualizations Using the Grammar of Graphics
library(ggrepel) # Automatically Position Non-Overlapping Text Labels with ‘ggplot2’
library(ggridges) # Ridgeline Plots in ‘ggplot2’
library(ggthemes) # Extra Themes, Scales, and Geoms for ‘ggplot2’
library(gmodels) # Various R Programming Tools for Model Fitting
library(grid) # The Grid Graphics Package
library(gridExtra)
library(knitr) # A General-Purpose Package for Dynamic Report Generation in R
library(lmtest) # Testing Linear Regression Models
library(plyr) # Tools for Splitting, Applying and Combining Data
library(purrr) # Functional Programming Tools
library(readr) # Read Rectangular Text Data
library(rpart)
library(rattle)
library(scales) # Scale Functions for Visualization
library(shiny) # Web Application Framework for R
library(skimr) # Compact and Flexible Summaries of Data
library(stringr) # Simple, Consistent Wrappers for Common String Operations
library(tibble) # Simple Data Frames
library(tidyr) # Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions
library(tidyverse) # Install and Load ‘Tidyverse’ Packages
library(usmap)
library(rworldmap)
library(ggmap)
library(maps)
library(mapdata)
library(cowplot)
library("googleway")
library("ggspatial") 
library("rnaturalearth") 
library("rnaturalearthdata")
theme_set(theme_bw())
library("sf")
library(gridExtra)

world <- ne_countries(scale = "medium", returnclass = "sf") #for mapping
```

Load cleaned data
```{r}
#clean_data <- readRDS("./data/processed_data/processeddata.rds")
clean_data <- readRDS("../../data/processed_data/processeddata.rds")
skim(clean_data)
```

```{r}
names(clean_data)
```

# Bioregion
```{r}
bioregion_histogram <- clean_data %>% ggplot() + geom_bar(aes(bioregion))
bioregion_histogram 
ggsave(filename = "../../results/Categorical_Outcome_Modeling_results/bioregion_histogram.png",plot =bioregion_histogram , width = 5, height = 4)
```

Data visualization
#write code that produces plots showing our outcome of interest on the x-axis and each numeric predictor on the y-axis.

```{r}
numer_data <- clean_data %>% select_if(is.numeric) 

numer_data$bioregion <- clean_data$bioregion

numer_data <- numer_data %>% dplyr::select(-size_bin)

numer_data_plot <- numer_data %>% gather(-bioregion, key = "var", value = "value") %>% 
            ggplot() + geom_point(aes(x = bioregion, y=value, col=bioregion)) +
            facet_wrap(~ var, scales = "free") + theme_bw() + theme(legend.position = "none")
print(numer_data_plot)
```

```{r}
categ_data <- clean_data %>% select_if(is.factor) 
categ_data <- categ_data %>% dplyr::select(-marine_site_name)
#names(categ_data)
categ_data_plotA <- categ_data %>% gather(-bioregion, -method_code, -species_code, -mpa_region, -georegion, -island, -group_code_UCSC_other, -method_code_IP_other, -state, key = "var", value = "value") %>% 
            ggplot() + geom_count(aes(x = bioregion, y=value, col=bioregion)) +
            facet_wrap(~ var, scales = "free") + theme_bw() + theme(legend.position = "none")
print(categ_data_plotA)
```

```{r}
categ_data_plotB <- categ_data %>% gather(-bioregion, -group_code, -site_code, -marine_season_code, -season_name, -island, -group_code_UCSC_other, -method_code_IP_other, -state, key = "var", value = "value") %>% 
            ggplot() + geom_count(aes(x = bioregion, y=value, col=bioregion)) +
            facet_wrap(~ var, scales = "free") + theme_bw() + theme(legend.position = "none")
print(categ_data_plotB)
```

```{r}
categ_data_plotC <- categ_data %>% gather(-bioregion, -group_code, -site_code, -marine_season_code, -season_name, -method_code, -species_code, -mpa_region, -georegion, key = "var", value = "value") %>% 
            ggplot() + geom_count(aes(x = bioregion, y=value, col=bioregion)) + 
            facet_wrap(~ var, scales = "free") + theme_bw() + theme(legend.position = "none")
print(categ_data_plotC)
```

Make sure Bioregion is first
```{r}
#names(clean_data) # bioregion is 19, 23 total, get rid of 3 and 15
clean_data_final <- clean_data[c(19,1,2,4,5,6,7,8,9,10,11,12,13,14,16,17,18,20,21,22,23)]
#names(clean_data_final) # double checking
```


Data splitting
```{r}
#write code that splits data into 70/30 train/test
#call the 2 parts data_train and data_test
library(caret)
set.seed(123)
trainset_treefit <- caret::createDataPartition(y = clean_data_final$bioregion, p = 0.7, list = FALSE)
data_train_treefit = clean_data_final[trainset_treefit,] #extract observations/rows for training, assign to new variable
data_test_treefit = clean_data_final[-trainset_treefit,] #do the same for the test set
```

Model Fitting
Null Model
```{r}
library("mlr")
null_accuracy_treefit <- measureACC("SanFran_2_GovtPt", clean_data_final$bioregion)
null_accuracy_treefit
```

Single predictor models
```{r}
#There is probably a nicer tidyverse way of doing this. I just couldn't think of it, so did it this way.
set.seed(1111) #makes each code block reproducible
outcomename = "bioregion"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #setting CV method for caret
Npred <- ncol(data_train_treefit)-1 # number of predictors
resultmat <- data.frame(Variable = names(data_train_treefit)[-1], Accuracy = rep(0,Npred)) #store performance for each variable
for (n in 2:ncol(data_train_treefit)) #loop over each predictor. For this to work, outcome must be in 1st column
{
  fit1 <- caret::train( as.formula(paste(outcomename, "~",names(data_train_treefit)[n])), data = data_train_treefit, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit1$results$Accuracy)  
}
print(resultmat)
```

So a lot of these have very strong (and in some cases, perfect, aka 100%) correlation with `bioregion`, and for very obvious reasons. Therefore, I'm going to remove variables in 2 ways: one will be more conservative (aka I'll be removing those with above a 90% accuracy- calling it `data_subset_A`) and one will be less conservative (aka I'll be removing those with above a 70% accuracy- calling it `data_subset_B`)

So lets do that now.
# Preparing Data Subset A
```{r}
data_subsetA <- clean_data_final %>% dplyr::select(-marine_sort_order, -latitude, -longitude, -mpa_region, -georegion)
```

# Data Splitting: Data Subset A
```{r}
#write code that splits data into 70/30 train/test
#call the 2 parts data_train and data_test
library(caret)
set.seed(123)
trainset_subsetA <- caret::createDataPartition(y = data_subsetA$bioregion, p = 0.7, list = FALSE)
data_train_subsetA = data_subsetA[trainset_subsetA,] #extract observations/rows for training, assign to new variable
data_test_subsetA = data_subsetA[-trainset_subsetA,] #do the same for the test set
```

Model Fitting
Null Model- Data Subset A
```{r}
library("mlr")
null_accuracy_subsetA <- measureACC("SanFran_2_GovtPt", data_subsetA$bioregion)
null_accuracy_subsetA
```

Null Model- Data Subset A
```{r}
library("mlr")
null_accuracy_subsetB <- measureACC("SanFran_2_GovtPt", data_subsetB$bioregion)
null_accuracy_subsetB
```

Single predictor models- Data Subset A
```{r}
#There is probably a nicer tidyverse way of doing this. I just couldn't think of it, so did it this way.
set.seed(1111) #makes each code block reproducible
outcomename = "bioregion"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #setting CV method for caret
Npred <- ncol(data_train_subsetA)-1 # number of predictors
resultmat <- data.frame(Variable = names(data_train_subsetA)[-1], Accuracy = rep(0,Npred)) #store performance for each variable
for (n in 2:ncol(data_train_subsetA)) #loop over each predictor. For this to work, outcome must be in 1st column
{
  fit1_subsetA <- caret::train(as.formula(paste(outcomename, "~",names(data_train_subsetA)[n])), data = data_train_subsetA, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit1_subsetA$results$Accuracy)  
}
print(resultmat)
```

Full model- Data Subset A
```{r}
set.seed(1111) #makes each code block reproducible
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
fit1_subsetA = caret::train(bioregion  ~ ., data=data_train_subsetA, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 
print(fit1_subsetA$results)
```

```{r}
library(rpart.plot)
prp(fit1_subsetA$finalModel, extra = 1, type = 1)
```

# Preparing Data Subset B
```{r}
data_subsetB <- clean_data_final %>% dplyr::select(-marine_sort_order, -latitude, -longitude, -mpa_region, -georegion, -site_code, -marine_common_season)
```

# Data Splitting: Data Subset B
```{r}
#write code that splits data into 70/30 train/test
#call the 2 parts data_train and data_test
library(caret)
set.seed(123)
trainset_subsetB <- caret::createDataPartition(y = data_subsetB$bioregion, p = 0.7, list = FALSE)
data_train_subsetB = data_subsetB[trainset_subsetB,] #extract observations/rows for training, assign to new variable
data_test_subsetB = data_subsetB[-trainset_subsetB,] #do the same for the test set
```

Single predictor models- Data Subset B
```{r}
#There is probably a nicer tidyverse way of doing this. I just couldn't think of it, so did it this way.
set.seed(1111) #makes each code block reproducible
outcomename = "bioregion"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #setting CV method for caret
Npred <- ncol(data_train_subsetB)-1 # number of predictors
resultmat <- data.frame(Variable = names(data_train_subsetB)[-1], Accuracy = rep(0,Npred)) #store performance for each variable
for (n in 2:ncol(data_train_subsetB)) #loop over each predictor. For this to work, outcome must be in 1st column
{
  fit1_subsetB <- caret::train(as.formula(paste(outcomename, "~",names(data_train_subsetB)[n])), data = data_train_subsetB, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit1_subsetB$results$Accuracy)  
}
print(resultmat)
```


Full model- Data Subset B
```{r}
set.seed(1111) #makes each code block reproducible
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
fit1_subsetB = caret::train(bioregion  ~ ., data=data_train_subsetB, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 
print(fit1_subsetB$results)
```

```{r}
library(rpart.plot)
prp(fit1_subsetB$finalModel, extra = 1, type = 1)
```


I'm going to continue on with just the Subset_B

Random forest
```{r}
set.seed(1111) #makes each code block reproducible
tuning_grid <- expand.grid( .mtry = seq(1,7,by=1), .splitrule = "gini", .min.node.size = seq(2,8,by=1) )
fit2 = caret::train(bioregion ~ ., data=data_train_subsetB, method="ranger", trControl = fitControl, tuneGrid = tuning_grid, na.action = na.pass) 
```

```{r}
plot(fit2)
```


Boosted tree ensemble
```{r}
library(gbm)
gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2), n.trees = 300, shrinkage = c(0.1, 0.01), n.minobsinnode = c(2,4,6))
fit3 = caret::train(bioregion ~ ., data=data_train_subsetB, method="gbm", trControl = fitControl, verbose=FALSE, tuneGrid = gbmGrid) 
plot(fit3)
```

Random forest with pre-processed predictors
```{r}
# copy the random forest code from above. Add a statement to the train() function that centers and scales predictors.
# save the result as fit4. 
set.seed(1111) #makes each code block reproducible
tuning_grid <- expand.grid( .mtry = seq(1,7,by=1), .splitrule = "gini", .min.node.size = seq(2,8,by=1) )
fit4 = caret::train(bioregion ~ ., preProcess = c("center", "scale"), data=data_train_subsetB, method="ranger",  trControl = fitControl, tuneGrid = tuning_grid, na.action = na.pass) 
plot(fit4)
```

Discriminant analysis
Don’t know if I should use method=“pda” or method=“pda2”, so I’m doing both.
```{r}
#write code that trains a pda model, use tuneLength 20. Save as fit5 and plot.
set.seed(1111) #makes each code block reproducible
tuning_grid <- expand.grid(lambda = seq(0,1,by=0.01))
fit5 = caret::train(bioregion ~ ., data=data_train_reduced, method="pda",  trControl = fitControl, tuneGrid = tuning_grid, na.action = na.pass, tuneLength = 20) 
#plot model results.
plot(fit5)
```



